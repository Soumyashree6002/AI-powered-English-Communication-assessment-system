{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1DWNw9jotr0AEmViySO3kIk8Q3us4RD8g",
      "authorship_tag": "ABX9TyOc1NQ/wVhqZ4bKAGkeXBpw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required libraries"
      ],
      "metadata": {
        "id": "8SfgSAv85jWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "T8HMVcRVlssF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "xNT0nRiBMUnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praat-parselmouth"
      ],
      "metadata": {
        "id": "1J8AuCxPQE8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy librosa TTS transformers mediapipe opencv-python-headless"
      ],
      "metadata": {
        "id": "aCcnhlktBvdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "AwfKyaINBiuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python protobuf"
      ],
      "metadata": {
        "id": "ICJzGgKrRB9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall mediapipe -y\n",
        "!pip install mediapipe --no-cache-dir"
      ],
      "metadata": {
        "id": "1NjEngn4R0-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "5FMQBBOPPvNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install edge-tts"
      ],
      "metadata": {
        "id": "BQW6Vd8o9M4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade mediapipe opencv-python-headless"
      ],
      "metadata": {
        "id": "pqVeHXJICetc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "dG7g9rKW6C12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required Libraries"
      ],
      "metadata": {
        "id": "z32RBXZH5xVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "import subprocess\n",
        "import re\n",
        "import ffmpeg\n",
        "import spacy\n",
        "import requests\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "N2I8xlA_mWo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There are different whisper models from tiny to large. While with increasing size complexity the accuracy of transcription increases, the time taken for transcripting the text also increases. So loading the small model of whisper for getting good transcription in less time. Basically a good trade off between time and accuracy."
      ],
      "metadata": {
        "id": "uYPq5EzE54hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"small\")"
      ],
      "metadata": {
        "id": "RWbUYxIjnzQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is the path for the input video that I used. One should change the path to a different one for the input video he/she will provide."
      ],
      "metadata": {
        "id": "OKNqLEsC6kEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_video = \"/content/WIN_20241211_21_41_16_Pro.mp4\""
      ],
      "metadata": {
        "id": "DEb-iK2GXQmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_video_to_audio(video_path, audio_output_path):\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        audio.write_audiofile(audio_output_path, codec='pcm_s16le')\n",
        "\n",
        "        print(f\"Audio file has been saved to: {audio_output_path}\")\n",
        "\n",
        "        video.close()\n",
        "        audio.close()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "video_path = input_video\n",
        "audio_output_path = \"audio.wav\"\n",
        "convert_video_to_audio(video_path, audio_output_path)\n"
      ],
      "metadata": {
        "id": "iI8T-UiCP7v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the model to convert speech to text"
      ],
      "metadata": {
        "id": "kmfX_Mv_c6uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"audio.wav\")\n",
        "transcription = result['text']\n",
        "\n",
        "print(f\"Transcription: {transcription}\")"
      ],
      "metadata": {
        "id": "jPjFfQN2n_a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checking for grammatical errors. Since the tool used also considers proper nouns as grammatical errors, we excluded all the proper nouns from the total errors"
      ],
      "metadata": {
        "id": "Y3orgPcvc-q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "url = \"https://api.languagetool.org/v2/check\"\n",
        "\n",
        "text = transcription\n",
        "\n",
        "params = {\n",
        "    'text': text,\n",
        "    'language': 'en-GB',\n",
        "}\n",
        "\n",
        "response = requests.post(url, data=params)\n",
        "\n",
        "result = response.json()\n",
        "\n",
        "def contains_proper_noun(match, nlp):\n",
        "    doc = nlp(match['context']['text'])\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PROPN':\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nErrors and Suggestions:\")\n",
        "\n",
        "\n",
        "for match in result['matches']:\n",
        "    print(f\"Error: {match['message']}\")\n",
        "    print(f\"Suggested Correction(s): {match['replacements']}\")\n",
        "    print(f\"Context: {match['context']['text']}\")\n",
        "\n",
        "error_count = 0\n",
        "\n",
        "for match in result['matches']:\n",
        "    if not contains_proper_noun(match, nlp):\n",
        "        error_count += 1\n",
        "\n",
        "print(f\"\\nTotal Errors (excluding proper nouns): {error_count}\")"
      ],
      "metadata": {
        "id": "PTLlNcMM6OAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checking for pauses and speaking rate\n"
      ],
      "metadata": {
        "id": "O6mmZWrkdFVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sound = parselmouth.Sound(\"audio.wav\")\n",
        "\n",
        "duration = call(sound, \"Get total duration\")\n",
        "\n",
        "silences = call(sound, \"To TextGrid (silences)\", 100, 0.1, -25, 0.1, 0.05, \"silent\", \"sounding\")\n",
        "\n",
        "n_pauses = call(silences, \"Count intervals where\", 1, \"is equal to\", \"silent\")\n",
        "\n",
        "n_words = len(transcription.split())\n",
        "speaking_rate = n_words / duration\n",
        "\n",
        "print(f\"Total Duration: {duration} seconds\")\n",
        "print(f\"Number of Pauses: {n_pauses}\")\n",
        "print(f\"Speaking Rate: {speaking_rate} words/second\")\n"
      ],
      "metadata": {
        "id": "_V_HV0PwP5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checking for filler words"
      ],
      "metadata": {
        "id": "dW32A0LzdJqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filler_words = [\"uh\", \"um\", \"like\", \"you know\", \"er\", \"ah\", \"so\", \"actually\", \"basically\"]\n",
        "\n",
        "pattern = r'\\b(' + '|'.join(filler_words) + r')\\b'\n",
        "\n",
        "matches = re.findall(pattern, transcription, flags=re.IGNORECASE)\n",
        "\n",
        "print(f\"Filler words found: {matches}\")\n",
        "\n",
        "word_counts = Counter(matches)\n",
        "print(f\"Filler word counts: {word_counts}\")\n",
        "filler_word_count = sum(word_counts.values())\n",
        "print(f\"total number of filler words: {filler_word_count}\")"
      ],
      "metadata": {
        "id": "vSjWVGtJXclK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the pronunciation by comparing original audio to the synthesised audio. Here we used edge text-to-speech model as it has more human like tone and also free of cost."
      ],
      "metadata": {
        "id": "rw65QGtc84l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synthesize_audio_from_text(text_input, output_file, gender):\n",
        "    try:\n",
        "        voice = \"en-IN-NeerjaNeural\" if gender == \"female\" else \"en-IN-PrabhatNeural\"\n",
        "        command = f'edge-tts --text \"{text_input}\" --write-media \"{output_file}\" --voice \"{voice}\"'\n",
        "        subprocess.run(command, shell=True, check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error synthesizing audio: {e}\")\n",
        "        return None\n",
        "    return output_file\n",
        "\n",
        "def compare_audio_files(original_audio_file, synthesized_audio_file):\n",
        "    try:\n",
        "        original_audio, sr_original = librosa.load(original_audio_file, sr=None)\n",
        "        synthesized_audio, sr_synthesized = librosa.load(synthesized_audio_file, sr=None)\n",
        "\n",
        "        mfcc_original = librosa.feature.mfcc(y=original_audio, sr=sr_original, n_mfcc=13)\n",
        "        mfcc_synthesized = librosa.feature.mfcc(y=synthesized_audio, sr=sr_synthesized, n_mfcc=13)\n",
        "\n",
        "        distance = cdist(mfcc_original.T, mfcc_synthesized.T, 'cosine')\n",
        "        avg_distance = np.mean(distance)\n",
        "\n",
        "        speech_rate_original = len(original_audio) / sr_original\n",
        "        speech_rate_synthesized = len(synthesized_audio) / sr_synthesized\n",
        "\n",
        "        pitch_original = librosa.yin(original_audio, fmin=librosa.note_to_hz('C1'), fmax=librosa.note_to_hz('C8'))\n",
        "        pitch_synthesized = librosa.yin(synthesized_audio, fmin=librosa.note_to_hz('C1'), fmax=librosa.note_to_hz('C8'))\n",
        "        pitch_variation_original = np.std(pitch_original)\n",
        "        pitch_variation_synthesized = np.std(pitch_synthesized)\n",
        "\n",
        "        result = {\n",
        "            \"avg_distance\": avg_distance,\n",
        "            \"speech_rate_original\": speech_rate_original,\n",
        "            \"speech_rate_synthesized\": speech_rate_synthesized,\n",
        "            \"pitch_variation_original\": pitch_variation_original,\n",
        "            \"pitch_variation_synthesized\": pitch_variation_synthesized,\n",
        "        }\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing audio files: {e}\")\n",
        "        return None\n",
        "\n",
        "def comp_pronun(transcription, gender=\"female\"):\n",
        "    transcription_text = transcription\n",
        "    original_audio_input = \"audio.wav\"\n",
        "    synthesized_audio_output = \"synthesized_audio.wav\"\n",
        "\n",
        "    if not synthesize_audio_from_text(transcription_text, synthesized_audio_output, gender):\n",
        "        return None\n",
        "\n",
        "    result = compare_audio_files(original_audio_input, synthesized_audio_output)\n",
        "    return result\n",
        "\n",
        "gender = \"female\"\n",
        "result_dict = comp_pronun(transcription, gender)\n",
        "if result_dict:\n",
        "    print(result_dict)"
      ],
      "metadata": {
        "id": "knCzHqwn6bCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the posture, gestures and eye-contact by using open-cv and mediapipe"
      ],
      "metadata": {
        "id": "BHW1dGmT9hJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "class BodyLanguageScorer:\n",
        "    def __init__(self):\n",
        "        self.scores = {\"posture\": 0, \"gestures\": 0, \"eye_contact\": 100}\n",
        "        self.gesture_count = 0\n",
        "        self.eye_movement_threshold = 0.05\n",
        "\n",
        "    def analyze(self, pose_landmarks, hand_landmarks, face_landmarks):\n",
        "        # Posture analysis\n",
        "        if pose_landmarks:\n",
        "            left_shoulder = pose_landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "            right_shoulder = pose_landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "            shoulder_diff = abs(left_shoulder.y - right_shoulder.y)\n",
        "            self.scores[\"posture\"] = max(0, 1 - shoulder_diff) * 100\n",
        "\n",
        "        if hand_landmarks:\n",
        "            self.gesture_count += len(hand_landmarks)\n",
        "            self.scores[\"gestures\"] = self.gesture_count * 25\n",
        "\n",
        "        if face_landmarks:\n",
        "            left_eye = face_landmarks[0].landmark[33]\n",
        "            right_eye = face_landmarks[0].landmark[133]\n",
        "            eye_distance = np.linalg.norm([left_eye.x - right_eye.x, left_eye.y - right_eye.y])\n",
        "            if eye_distance > self.eye_movement_threshold:\n",
        "                self.scores[\"eye_contact\"] = max(0, 100 - eye_distance * 100)\n",
        "\n",
        "    def get_scores(self):\n",
        "        return self.scores\n",
        "\n",
        "def process_video_optimized(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    scorer = BodyLanguageScorer()\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_skip = max(1, total_frames // 200)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
        "         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands, \\\n",
        "         mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
        "\n",
        "        for frame_idx in range(0, total_frames, frame_skip):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            image.flags.writeable = False\n",
        "\n",
        "            pose_results = pose.process(image)\n",
        "            hands_results = hands.process(image)\n",
        "            face_results = face_mesh.process(image)\n",
        "\n",
        "            scorer.analyze(\n",
        "                pose_results.pose_landmarks.landmark if pose_results.pose_landmarks else None,\n",
        "                hands_results.multi_hand_landmarks,\n",
        "                face_results.multi_face_landmarks\n",
        "            )\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    scores = scorer.get_scores()\n",
        "    if scores[\"posture\"] < 60 or scores[\"eye_contact\"] < 70:\n",
        "        print(\"Warning: Poor posture or eye contact detected!\")\n",
        "\n",
        "    print(\"Final Scores:\")\n",
        "    print(f\"Posture: {scores['posture']:.2f}\")\n",
        "    print(f\"Gestures: {scores['gestures']:.2f}\")\n",
        "    print(f\"Eye Contact: {scores['eye_contact']:.2f}\")\n",
        "\n",
        "video_path = input_video\n",
        "process_video_optimized(video_path)\n"
      ],
      "metadata": {
        "id": "rBqCecxB2qfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the pronunciation score"
      ],
      "metadata": {
        "id": "x5AaHeCq91EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x, min_val, max_val):\n",
        "    return (x - min_val) / (max_val - min_val)\n",
        "\n",
        "def calculate_pronunciation_score(mfcc_dist,\n",
        "                                  word_rate_actual, word_rate_ideal,\n",
        "                                  pitch_var_actual, pitch_var_ideal,\n",
        "                                  mfcc_range=(0, 5),\n",
        "                                  word_rate_range=(0, 10),\n",
        "                                  pitch_var_range=(0, 4000),\n",
        "                                  weights=(0.5, 0.3, 0.2)):\n",
        "    mfcc_norm = normalize(mfcc_dist, *mfcc_range)\n",
        "\n",
        "    word_rate_diff = abs(word_rate_actual - word_rate_ideal)\n",
        "    pitch_var_diff = abs(pitch_var_actual - pitch_var_ideal)\n",
        "\n",
        "    word_rate_score = 1 - normalize(word_rate_diff, 0, word_rate_range[1])  # Smaller diff is better\n",
        "    pitch_var_score = 1 - normalize(pitch_var_diff, 0, pitch_var_range[1])  # Smaller diff is better\n",
        "\n",
        "    pronunciation_score = (\n",
        "        weights[0] * (1 - mfcc_norm) +  # 1 - mfcc_norm since lower distance is better\n",
        "        weights[1] * word_rate_score +\n",
        "        weights[2] * pitch_var_score\n",
        "    )\n",
        "\n",
        "    final_score = pronunciation_score * 100\n",
        "    return round(final_score, 2)\n",
        "\n",
        "mfcc_dist = result_dict['avg_distance']\n",
        "word_rate_actual = result_dict['speech_rate_original']\n",
        "word_rate_ideal = result_dict['speech_rate_synthesized']\n",
        "pitch_var_actual = result_dict['pitch_variation_original']\n",
        "pitch_var_ideal = result_dict['pitch_variation_synthesized']\n",
        "\n",
        "pronunciation_score = calculate_pronunciation_score(\n",
        "    mfcc_dist,\n",
        "    word_rate_actual, word_rate_ideal,\n",
        "    pitch_var_actual, pitch_var_ideal,\n",
        "    mfcc_range=(0, 5),\n",
        "    word_rate_range=(0, 10),\n",
        "    pitch_var_range=(0, 3500)\n",
        ")\n",
        "\n",
        "print(pronunciation_score)"
      ],
      "metadata": {
        "id": "mTyyKDznOuGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating overall score"
      ],
      "metadata": {
        "id": "UsyBJbQk96x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy_score(pronunciation_score, error_count, speaking_rate, pause_count, filler_word_count):\n",
        "\n",
        "    weights = {\n",
        "        \"pronunciation\": 0.4,\n",
        "        \"error\": 0.2,\n",
        "        \"speaking_rate\": 0.2,\n",
        "        \"pause\": 0.1,\n",
        "        \"filler_word\": 0.1,\n",
        "    }\n",
        "\n",
        "    normalized_pronunciation = pronunciation_score / 100\n",
        "\n",
        "    normalized_error = max(0, 1 - error_count / 20)\n",
        "\n",
        "    if 2 <= speaking_rate <= 4:\n",
        "        normalized_speaking_rate = 1\n",
        "    else:\n",
        "        normalized_speaking_rate = max(0, 1 - abs(speaking_rate - 3) / 3)\n",
        "\n",
        "    normalized_pause = max(0, 1 - pause_count / 10)\n",
        "\n",
        "    normalized_filler = max(0, 1 - filler_word_count / 10)\n",
        "\n",
        "    score = (\n",
        "        weights[\"pronunciation\"] * normalized_pronunciation +\n",
        "        weights[\"error\"] * normalized_error +\n",
        "        weights[\"speaking_rate\"] * normalized_speaking_rate +\n",
        "        weights[\"pause\"] * normalized_pause +\n",
        "        weights[\"filler_word\"] * normalized_filler\n",
        "    )\n",
        "\n",
        "    return round(score * 100, 2)\n",
        "\n",
        "accuracy_score = calculate_accuracy_score(\n",
        "    pronunciation_score, error_count, speaking_rate, n_pauses, filler_word_count)\n",
        "\n",
        "print(f\"Score: {accuracy_score}\")"
      ],
      "metadata": {
        "id": "-QBOpRVGVRu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufytt8VudQDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}